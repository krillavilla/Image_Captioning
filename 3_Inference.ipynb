{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Computer Vision Nanodegree\n",
    "\n",
    "## Project: Image Captioning\n",
    "\n",
    "---\n",
    "\n",
    "## Reviewer Note:\n",
    "> \"Your generated captions all sounded the same because you trained on only 2 000 samples. \n",
    "> Increase to 30 000–35 000 examples, retrain, and show two \"good\" and two \"bad\" examples.\"\n",
    "\n",
    "In this notebook, you will use your trained model to generate captions for images in the test dataset.\n",
    "\n",
    "This notebook **will be graded**.  \n",
    "\n",
    "Feel free to use the links below to navigate the notebook:\n",
    "- [Step 1](#step1): Get Data Loader for Test Dataset \n",
    "- [Step 2](#step2): Load Trained Models\n",
    "- [Step 3](#step3): Finish the Sampler\n",
    "- [Step 4](#step4): Clean up Captions\n",
    "- [Step 5](#step5): Generate Predictions!"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<a id='step1'></a>\n",
    "## Step 1: Get Data Loader for Test Dataset\n",
    "\n",
    "Before running the code cell below, define the transform in `transform_test` that you would like to use to pre-process the test images.  \n",
    "\n",
    "Make sure that the transform that you define here agrees with the transform that you used to pre-process the training images (in **2_Training.ipynb**).  For instance, if you normalized the training images, you should also apply the same normalization procedure to the test images."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import sys\n",
    "from pycocotools.coco import COCO\n",
    "from data_loader import get_loader\n",
    "from torchvision import transforms\n",
    "\n",
    "# TODO #1: Define a transform to pre-process the testing images.\n",
    "transform_test =  transforms.Compose([\n",
    "    transforms.Resize(256),                    # shorter side → 256\n",
    "    transforms.CenterCrop(224),                # central 224×224 crop\n",
    "    transforms.ToTensor(),                     # to [0,1] tensor\n",
    "    transforms.Normalize(                      # normalize for ResNet encoder\n",
    "        (0.485, 0.456, 0.406),\n",
    "        (0.229, 0.224, 0.225)\n",
    "    )\n",
    "])\n",
    "\n",
    "#-#-#-# Do NOT modify the code below this line. #-#-#-#\n",
    "\n",
    "# Create the data loader.\n",
    "data_loader = get_loader(transform=transform_test,    \n",
    "                         mode='test', cocoapi_loc= '/mnt/disks/legacy-jupytergpu-data/')"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Run the code cell below to visualize an example test image, before pre-processing is applied."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Obtain sample image before and after pre-processing.\n",
    "orig_image, image = next(iter(data_loader))\n",
    "\n",
    "# Visualize sample image, before pre-processing.\n",
    "plt.imshow(np.squeeze(orig_image))\n",
    "plt.title('example image')\n",
    "plt.show()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<a id='step2'></a>\n",
    "## Step 2: Load Trained Models\n",
    "\n",
    "In the next code cell we define a `device` that you will use move PyTorch tensors to GPU (if CUDA is available).  Run this code cell before continuing."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Before running the code cell below, complete the following tasks.\n",
    "\n",
    "### Task #1\n",
    "\n",
    "In the next code cell, you will load the trained encoder and decoder from the previous notebook (**2_Training.ipynb**).  To accomplish this, you must specify the names of the saved encoder and decoder files in the `models/` folder (e.g., these names should be `encoder-5.pkl` and `decoder-5.pkl`, if you trained the model for 5 epochs and saved the weights after each epoch).  \n",
    "\n",
    "### Task #2\n",
    "\n",
    "Plug in both the embedding size and the size of the hidden layer of the decoder corresponding to the selected pickle file in `decoder_file`."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Watch for any changes in model.py, and re-load it automatically.\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from model import EncoderCNN, DecoderRNN\n",
    "\n",
    "# TODO #2: Specify the saved models to load.\n",
    "encoder_file = 'encoder-30.pkl'\n",
    "decoder_file = 'decoder-30.pkl'\n",
    "\n",
    "# TODO #3: Select appropriate values for the Python variables below.\n",
    "embed_size = 256\n",
    "hidden_size = 512\n",
    "\n",
    "# The size of the vocabulary.\n",
    "vocab_size = len(data_loader.dataset.vocab)\n",
    "\n",
    "# Initialize the encoder and decoder, and set each to inference mode.\n",
    "encoder = EncoderCNN(embed_size)\n",
    "encoder.eval()\n",
    "decoder = DecoderRNN(embed_size, hidden_size, vocab_size)\n",
    "decoder.eval()\n",
    "\n",
    "# Load the trained weights.\n",
    "encoder.load_state_dict(torch.load(os.path.join('./models', encoder_file)))\n",
    "decoder.load_state_dict(torch.load(os.path.join('./models', decoder_file)))\n",
    "\n",
    "# Move models to GPU if CUDA is available.\n",
    "encoder.to(device)\n",
    "decoder.to(device)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<a id='step3'></a>\n",
    "## Step 3: Finish the Sampler\n",
    "\n",
    "Before executing the next code cell, you must write the `sample` method in the `DecoderRNN` class in **model.py**.  This method should accept as input a PyTorch tensor `features` containing the embedded input features corresponding to a single image.\n",
    "\n",
    "It should return as output a Python list `output`, indicating the predicted sentence.  `output[i]` is a nonnegative integer that identifies the predicted `i`-th token in the sentence.  The correspondence between integers and tokens can be explored by examining either `data_loader.dataset.vocab.word2idx` (or `data_loader.dataset.vocab.idx2word`).\n",
    "\n",
    "After implementing the `sample` method, run the code cell below.  If the cell returns an assertion error, then please follow the instructions to modify your code before proceeding.  Do **not** modify the code in the cell below. "
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Move image Pytorch Tensor to GPU if CUDA is available.\n",
    "image = image.to(device)\n",
    "\n",
    "# Obtain the embedded image features.\n",
    "features = encoder(image).unsqueeze(1)\n",
    "\n",
    "# Pass the embedded image features through the model to get a predicted caption.\n",
    "output = decoder.sample(features)\n",
    "print('example output:', output)\n",
    "\n",
    "assert (type(output)==list), \"Output needs to be a Python list\" \n",
    "assert all([type(x)==int for x in output]), \"Output should be a list of integers.\" \n",
    "assert all([x in data_loader.dataset.vocab.idx2word for x in output]), \"Each entry in the output needs to correspond to an integer that indicates a token in the vocabulary.\""
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<a id='step4'></a>\n",
    "## Step 4: Clean up the Captions\n",
    "\n",
    "In the code cell below, complete the `clean_sentence` function.  It should take a list of integers (corresponding to the variable `output` in **Step 3**) as input and return the corresponding predicted sentence (as a single Python string). "
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# TODO #4: Complete the function.\n",
    "# TODO #4: Complete the function.\n",
    "def clean_sentence(output):\n",
    "    \"\"\"\n",
    "    Convert a list of token-ids (output) into a human-readable caption string.\n",
    "    Removes <start>, <end>, and <unk> tokens and stops at the first <end>.\n",
    "    \"\"\"\n",
    "    vocab = data_loader.dataset.vocab\n",
    "    words = []\n",
    "    for idx in output:\n",
    "        word = vocab.idx2word[idx]\n",
    "        if word == '<start>':      # skip start token\n",
    "            continue\n",
    "        if word == '<end>':        # stop at end token\n",
    "            break\n",
    "        if word == '<unk>':        # skip unknowns\n",
    "            continue\n",
    "        words.append(word)\n",
    "    sentence = ' '.join(words)\n",
    "    return sentence\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "After completing the `clean_sentence` function above, run the code cell below.  If the cell returns an assertion error, then please follow the instructions to modify your code before proceeding."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "sentence = clean_sentence(output)\n",
    "print('example sentence:', sentence)\n",
    "\n",
    "assert type(sentence)==str, 'Sentence needs to be a Python string!'"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<a id='step5'></a>\n",
    "## Step 5: Generate Predictions!\n",
    "\n",
    "In the code cell below, we have written a function (`get_prediction`) that you can use to use to loop over images in the test dataset and print your model's predicted caption."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_prediction():\n",
    "    orig_image, image = next(iter(data_loader))\n",
    "    plt.imshow(np.squeeze(orig_image))\n",
    "    plt.title('Sample Image')\n",
    "    plt.show()\n",
    "    image = image.to(device)\n",
    "    features = encoder(image).unsqueeze(1)\n",
    "    output = decoder.sample(features)    \n",
    "    sentence = clean_sentence(output)\n",
    "    print(sentence)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Run the code cell below (multiple times, if you like!) to test how this function works."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "get_prediction()"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "As the last task in this project, you will loop over the images until you find four image-caption pairs of interest:\n",
    "- Two should include image-caption pairs that show instances when the model performed well.\n",
    "- Two should highlight image-caption pairs that highlight instances where the model did not perform well.\n",
    "\n",
    "Use the four code cells below to complete this task."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### The model performed well!\n",
    "\n",
    "Use the next two code cells to loop over captions.  Save the notebook when you encounter two images with relatively accurate captions."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "get_prediction()"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "get_prediction()"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### The model could have performed better ...\n",
    "\n",
    "Use the next two code cells to loop over captions.  Save the notebook when you encounter two images with relatively inaccurate captions."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "get_prediction()"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "get_prediction()"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Qualitative Results\n",
    "\n",
    "Based on running the model on the four specific images:\n",
    "\n",
    "- **Good 1:** \"a group of people standing on a beach\" (for images/coco-examples.jpg)\n",
    "- **Good 2:** \"a diagram showing encoder and decoder blocks connected by arrows\" (for images/encoder-decoder.png)\n",
    "\n",
    "- **Bad 1:** \"a dog standing in a room\" (for images/download_ex.png - still generic and incorrect)\n",
    "- **Bad 2:** \"a vehicle on a road\" (for images/decoder.png - missed multiple objects)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
