# Image Captioning ğŸ–¼ï¸â¡ï¸ğŸ“

**Automatically describe what you see in an image**

---

## ğŸš€ Project Overview
This repository showcases a PyTorch-based **Image Captioning** system. Leveraging a pretrained ResNet-18 as an image encoder and an LSTM-based decoder, the model learns to translate image features into natural language descriptions. Perfect for accessibility applications, photo organization, or just for fun!

## ğŸ”‘ Key Features
- **CNN Encoder**: Uses pretrained ResNet-18 to extract rich visual features.
- **RNN Decoder**: An LSTM network that learns to generate human-readable captions.
- **End-to-End Training**: Trains on the MS COCO dataset for realistic, diverse descriptions.
- **Inference Pipeline**: Includes preprocessing, sampling, and `clean_sentence` to produce clean captions.
- **Modular Design**: Easily swap encoder backbones or adjust decoder depth.

## ğŸ“ Repository Structure
```
â”œâ”€â”€ 0_Dataset.ipynb        # Prepare and explore MS COCO data
â”œâ”€â”€ 1_Preliminaries.ipynb  # Setup transforms, data loader, and imports
â”œâ”€â”€ 2_Training.ipynb       # Train your Encoder-Decoder model
â”œâ”€â”€ 3_Inference.ipynb      # Generate captions for novel images
â”œâ”€â”€ data_loader.py         # COCO dataset wrapper and DataLoader
â”œâ”€â”€ vocabulary.py          # Build and manage the word vocabulary
â”œâ”€â”€ model.py               # EncoderCNN and DecoderRNN implementations
â”œâ”€â”€ models/                # Saved checkpoint files
â”œâ”€â”€ images/                # Example input/output visuals
â””â”€â”€ README.md              # Project overview and instructions
```

## âš™ï¸ Getting Started
1. **Enable GPU**: In the Udacity workspace, toggle GPU mode for faster training.
2. **Install dependencies**:
   ```bash
   pip install -r requirements.txt
   ```
3. **Download COCO data** (handled automatically if using workspace).
4. **Train**:
   - Run **2_Training.ipynb**, customize hyperparameters (batch size, learning rate, etc.).
   - Monitor loss and perplexity.
5. **Infer**:
   - Run **3_Inference.ipynb** to generate captions on new images.

## ğŸ“Š Results
Below is an example of model outputs on COCO test images:

| Image                                      | Generated Caption                         |
|--------------------------------------------|-------------------------------------------|
| ![Example](images/coco-examples.jpg)       | `A group of people sitting around a table` |

## ğŸ› ï¸ Customization
- **Change Encoder**: Try deeper ResNets or EfficientNetsâ€”just swap in a different model in `model.py`.
- **Adjust Decoder**: Increase `embed_size` or `hidden_size` for richer language models.
- **Data Augmentation**: Modify `transform_train` to include color jitter, rotation, or random crop scales.

## ğŸ“š References
- Vinyals et al., *â€œShow and Tell: A Neural Image Caption Generatorâ€* (ICLR 2015)
- Xu et al., *â€œShow, Attend and Tell: Neural Image Caption Generation with Visual Attentionâ€* (ICML 2015)

## ğŸ™ Acknowledgments
- MS COCO Team for providing the dataset.
- Udacity for the project framework and guidance.

---

